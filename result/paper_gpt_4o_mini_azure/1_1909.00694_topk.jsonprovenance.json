[
    {
        "provenance_id": 0,
        "input_sentence_ids": [
            14,
            15,
            16,
            17,
            18
        ],
        "provenance_ids": [
            18
        ],
        "provenance": "We transform this idea into objective functions and train neural network models that predict the polarity of a given event.",
        "time": 0.007853984832763672,
        "input_token_size": 4248,
        "output_token_size": 25
    },
    {
        "provenance_id": 1,
        "input_sentence_ids": [
            9,
            10,
            11,
            12,
            13
        ],
        "provenance_ids": [
            13
        ],
        "provenance": "Suppose that events $x_1$ are $x_2$ are in the discourse relation of Cause (i.e., $x_1$ causes $x_2$).",
        "time": 0.009620904922485352,
        "input_token_size": 4461,
        "output_token_size": 30
    },
    {
        "provenance_id": 2,
        "input_sentence_ids": [
            4,
            5,
            6,
            7,
            8
        ],
        "provenance_ids": [
            8
        ],
        "provenance": "In this paper, we work on recognizing the polarity of an affective event that is represented by a score ranging from $-1$ (negative) to 1 (positive).",
        "time": 0.01147007942199707,
        "input_token_size": 4877,
        "output_token_size": 40
    },
    {
        "provenance_id": 3,
        "input_sentence_ids": [
            0,
            1,
            2,
            3
        ],
        "provenance_ids": [
            3
        ],
        "provenance": "Our experiments using Japanese data show that our method learns affective events effectively without manually labeled data.",
        "time": 0.012881994247436523,
        "input_token_size": 4999,
        "output_token_size": 45
    },
    {
        "provenance_id": 4,
        "input_sentence_ids": [
            19,
            20,
            21,
            22
        ],
        "provenance_ids": [
            22
        ],
        "provenance": "Learning affective events is closely related to sentiment analysis.",
        "time": 0.014642000198364258,
        "input_token_size": 5751,
        "output_token_size": 60
    },
    {
        "provenance_id": 5,
        "input_sentence_ids": [
            23,
            24,
            25,
            26,
            27
        ],
        "provenance_ids": [
            27
        ],
        "provenance": "Label propagation from seed instances is a common approach to inducing sentiment polarities.",
        "time": 0.015678882598876953,
        "input_token_size": 5915,
        "output_token_size": 65
    },
    {
        "provenance_id": 6,
        "input_sentence_ids": [
            33,
            34,
            35,
            36,
            37
        ],
        "provenance_ids": [
            37
        ],
        "provenance": "Our method depends only on raw texts and thus has wider applicability.",
        "time": 0.016985177993774414,
        "input_token_size": 6283,
        "output_token_size": 75
    },
    {
        "provenance_id": 7,
        "input_sentence_ids": [
            28,
            29,
            30,
            31,
            32
        ],
        "provenance_ids": [
            32
        ],
        "provenance": "Compared with this method, our discourse relation-based linking of events is much simpler and more intuitive.",
        "time": 0.018010854721069336,
        "input_token_size": 6453,
        "output_token_size": 80
    },
    {
        "provenance_id": 8,
        "input_sentence_ids": [
            52,
            53,
            54,
            55,
            56
        ],
        "provenance_ids": [
            56
        ],
        "provenance": "We assume the two events have the same polarities.",
        "time": 0.02065587043762207,
        "input_token_size": 8190,
        "output_token_size": 100
    },
    {
        "provenance_id": 9,
        "input_sentence_ids": [
            47,
            48,
            49,
            50,
            51
        ],
        "provenance_ids": [
            51
        ],
        "provenance": "The seed lexicon matches (1) the latter event but (2) not the former event, and (3) their discourse relation type is Cause or Concession.",
        "time": 0.021657228469848633,
        "input_token_size": 8350,
        "output_token_size": 105
    },
    {
        "provenance_id": 10,
        "input_sentence_ids": [
            42,
            43,
            44,
            45,
            46
        ],
        "provenance_ids": [
            46
        ],
        "provenance": "As shown in Figure FIGREF1, we limit our scope to two discourse relations: Cause and Concession.",
        "time": 0.023127079010009766,
        "input_token_size": 8744,
        "output_token_size": 115
    },
    {
        "provenance_id": 11,
        "input_sentence_ids": [
            38,
            39,
            40,
            41
        ],
        "provenance_ids": [
            41
        ],
        "provenance": "${\\rm tanh}$ is the hyperbolic tangent and transforms the scalar into a score ranging from $-1$ to 1.",
        "time": 0.024249792098999023,
        "input_token_size": 8876,
        "output_token_size": 120
    },
    {
        "provenance_id": 12,
        "input_sentence_ids": [
            57,
            58,
            59,
            60
        ],
        "provenance_ids": [
            60
        ],
        "provenance": "We define a loss function for each of the three types of event pairs and sum up the multiple loss functions.",
        "time": 0.02606797218322754,
        "input_token_size": 9734,
        "output_token_size": 135
    },
    {
        "provenance_id": 13,
        "input_sentence_ids": [
            61,
            62,
            63,
            64,
            65
        ],
        "provenance_ids": [
            65
        ],
        "provenance": "For the CA data, the loss function is defined as:\n\n$y_{i1}$ and $y_{i2}$ are the $i$-th pair of the CA pairs.",
        "time": 0.027503013610839844,
        "input_token_size": 9911,
        "output_token_size": 140
    },
    {
        "provenance_id": 14,
        "input_sentence_ids": [
            71,
            72,
            73,
            74,
            75
        ],
        "provenance_ids": [
            75
        ],
        "provenance": "Here is an example of event pair extraction. .",
        "time": 0.029050827026367188,
        "input_token_size": 10366,
        "output_token_size": 150
    },
    {
        "provenance_id": 15,
        "input_sentence_ids": [
            66,
            67,
            68,
            69,
            70
        ],
        "provenance_ids": [
            70
        ],
        "provenance": "As a raw corpus, we used a Japanese web corpus that was compiled through the procedures proposed by BIBREF13.",
        "time": 0.030133962631225586,
        "input_token_size": 10501,
        "output_token_size": 155
    },
    {
        "provenance_id": 16,
        "input_sentence_ids": [
            109,
            110,
            111,
            112,
            113
        ],
        "provenance_ids": [
            113
        ],
        "provenance": "This suggests that the training set of 0.6 million events is sufficiently large for training the models.",
        "time": 0.03443408012390137,
        "input_token_size": 14177,
        "output_token_size": 180
    },
    {
        "provenance_id": 17,
        "input_sentence_ids": [
            104,
            105,
            106,
            107,
            108
        ],
        "provenance_ids": [
            108
        ],
        "provenance": "This demonstrates the effectiveness of discourse relation-based label propagation.",
        "time": 0.03541898727416992,
        "input_token_size": 14310,
        "output_token_size": 185
    },
    {
        "provenance_id": 18,
        "input_sentence_ids": [
            99,
            100,
            101,
            102,
            103
        ],
        "provenance_ids": [
            103
        ],
        "provenance": "As the Random baseline suggests, positive and negative labels were distributed evenly.",
        "time": 0.03678011894226074,
        "input_token_size": 14816,
        "output_token_size": 195
    },
    {
        "provenance_id": 19,
        "input_sentence_ids": [
            95,
            96,
            97,
            98
        ],
        "provenance_ids": [
            98
        ],
        "provenance": "Its output is the final hidden state corresponding to the special classification tag ([CLS]).",
        "time": 0.037580013275146484,
        "input_token_size": 14923,
        "output_token_size": 200
    }
]